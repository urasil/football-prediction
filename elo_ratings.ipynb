{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error, accuracy_score, precision_recall_fscore_support,mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_selection import SelectKBest, chi2, RFE, mutual_info_regression\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import norm\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"finalData.csv\"\n",
    "df = pd.read_csv(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season(date):\n",
    "    if date.month >= 8:  \n",
    "        return (date.year)\n",
    "    else:  \n",
    "        return (date.year - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming the columns\n",
    "df.columns = ['Date', 'Home Team', 'Away Team', 'Full Time Home Goals', 'Full Time Away Goals', 'Full Time Result',       \n",
    "'Half Time Home Goals', 'Half Time Away Goals', 'Half Time Result', 'Referee', 'Home Shots', 'Away Shots', 'Home Shots on Target',   \n",
    "'Away Shots on Target', 'Home Corners', 'Away Corners', 'Home Fouls', 'Away Fouls', 'Home Yellow Cards', 'Away Yellow Cards',     \n",
    "'Home Red Cards', 'Away Red Cards', 'Home Possession', 'Away Possession', 'Home Passes Completed', 'Home Passes PCT',\n",
    "'Home Progressive Passes', 'Home Progressive Passing Distance', 'Home xG', 'Home Take Ons Won', 'Home Take Ons', \n",
    "'Home Interceptions', 'Home Blocks', 'Home Touches', 'Home Touches Def 3rd', 'Home Touches Mid 3rd', 'Home Touches Att 3rd',\n",
    "'Home Carries', 'Home Carries Progressive Distance', 'Home Tackles', 'Home Tackles Won', 'Away Passes Completed',\n",
    "'Away Passes PCT', 'Away Progressive Passes', 'Away Progressive Passing Distance', 'Away xG',\n",
    "'Away Take Ons Won', 'Away Take Ons', 'Away Interceptions', 'Away Blocks', 'Away Touches', 'Away Touches Def 3rd',\n",
    "'Away Touches Mid 3rd', 'Away Touches Att 3rd', 'Away Carries', 'Away Carries Progressive Distance', 'Away Tackles',\n",
    "'Away Tackles Won']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure sorted by date\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n",
    "df = df.sort_values(by='Date')\n",
    "\n",
    "# each season starts at 08 and ends at 05 of next year - 2000-2001 season will be the 2000 season\n",
    "def get_season(date):\n",
    "    if date.month >= 8:  \n",
    "        return (date.year)\n",
    "    else:  \n",
    "        return (date.year - 1)\n",
    "\n",
    "df['Season'] = df['Date'].apply(get_season)\n",
    "df['Match Outcome'] = df['Full Time Result'].map({'H': 1, 'D': 0, 'A': -1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_season_points(df):\n",
    "    df['Home Team Points'] = 0\n",
    "    df['Away Team Points'] = 0\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        if row['Match Outcome'] == 1:  \n",
    "            df.at[idx, 'Home Team Points'] = 3\n",
    "            df.at[idx, 'Away Team Points'] = 0\n",
    "        elif row['Match Outcome'] == 0: \n",
    "            df.at[idx, 'Home Team Points'] = 1\n",
    "            df.at[idx, 'Away Team Points'] = 1\n",
    "        elif row['Match Outcome'] == -1:  \n",
    "            df.at[idx, 'Home Team Points'] = 0\n",
    "            df.at[idx, 'Away Team Points'] = 3\n",
    "\n",
    "    df['Home Total Seasonal Points'] = (df.groupby(['Home Team', 'Season'])['Home Team Points'].cumsum())\n",
    "    df['Away Total Seasonal Points'] = (df.groupby(['Away Team', 'Season'])['Away Team Points'].cumsum())\n",
    "    # df['Seasonal Point Difference'] = df['Home Total Seasonal Points'] - df['Away Total Seasonal Points']\n",
    "    # df.drop(columns=['Home Team Points', 'Away Team Points'], inplace=True)\n",
    "\n",
    "    return df\n",
    "df = calculate_season_points(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    # Home Advantage\n",
    "    \"Home Team\",\n",
    "    \"Away Team\",\n",
    "    \"Match Outcome\",\n",
    "    \"Full Time Home Goals\",\n",
    "    \"Full Time Away Goals\",\n",
    "    \n",
    "    # Attacking Strength\n",
    "    \"Home Shots on Target\", \"Away Shots on Target\",\n",
    "    \"Home Progressive Passes\", \"Away Progressive Passes\",\n",
    "    \"Home Touches Att 3rd\", \"Away Touches Att 3rd\",\n",
    "    \"Home Take Ons Won\", \"Away Take Ons Won\",\n",
    "    \"Home Corners\", \"Away Corners\",\n",
    "    \n",
    "    # Midfield Strength\n",
    "    \"Home Touches Mid 3rd\", \"Away Touches Mid 3rd\",\n",
    "    \"Home Passes Completed\", \"Away Passes Completed\",\n",
    "    \"Home Passes PCT\", \"Away Passes PCT\",\n",
    "    \"Home Carries\", \"Away Carries\",\n",
    "    \n",
    "    # Defensive Strength\n",
    "    \"Home Tackles\", \"Away Tackles\",\n",
    "    \"Home Tackles Won\", \"Away Tackles Won\",\n",
    "    \"Home Blocks\", \"Away Blocks\",\n",
    "    \"Home Interceptions\", \"Away Interceptions\",\n",
    "    \n",
    "    # Extra\n",
    "    \"Full Time Result\",\n",
    "    'Date',\n",
    "    'Season'\n",
    "]\n",
    "\n",
    "updated_df = df[features].copy()\n",
    "updated_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = [\n",
    "    # Home Advantage,\n",
    "    \"Full Time Home Goals\",\n",
    "    \"Full Time Away Goals\",\n",
    "    \n",
    "    # Attacking Strength\n",
    "    \"Home Shots on Target\", \"Away Shots on Target\",\n",
    "    \"Home Progressive Passes\", \"Away Progressive Passes\",\n",
    "    \"Home Touches Att 3rd\", \"Away Touches Att 3rd\",\n",
    "    \"Home Take Ons Won\", \"Away Take Ons Won\",\n",
    "    \"Home Corners\", \"Away Corners\",\n",
    "    \n",
    "    # Midfield Strength\n",
    "    \"Home Touches Mid 3rd\", \"Away Touches Mid 3rd\",\n",
    "    \"Home Passes Completed\", \"Away Passes Completed\",\n",
    "    \"Home Passes PCT\", \"Away Passes PCT\",\n",
    "    \"Home Carries\", \"Away Carries\",\n",
    "    \n",
    "    # Defensive Strength\n",
    "    \"Home Tackles\", \"Away Tackles\",\n",
    "    \"Home Tackles Won\", \"Away Tackles Won\",\n",
    "    \"Home Blocks\", \"Away Blocks\",\n",
    "    \"Home Interceptions\", \"Away Interceptions\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardisation(df, terms=numerical_features):\n",
    "    scaler = StandardScaler()\n",
    "    df[terms] = scaler.fit_transform(df[terms])\n",
    "    return df\n",
    "\n",
    "# normalises a specific set of columns\n",
    "def normalisation(df, terms=numerical_features):\n",
    "    scaler = MinMaxScaler()\n",
    "    df[terms] = scaler.fit_transform(df[terms])\n",
    "    return df\n",
    "\n",
    "# label encoding - use for boosting models\n",
    "def label_encoding(df, col=['Home Team', 'Away Team'], dataframe=True):\n",
    "    label_encoder = LabelEncoder()\n",
    "    if dataframe:\n",
    "        for c in col:\n",
    "            df[c] = label_encoder.fit_transform(df[c])\n",
    "    else:\n",
    "        df = label_encoder.fit_transform(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(df):\n",
    "    # drop the irrelevant terms\n",
    "    featuresToDrop = ['Date', 'Home Team', 'Away Team', 'Full Time Result', 'Half Time Result', 'Referee']\n",
    "    assert set(featuresToDrop).issubset(df.columns), \"Some columns in featuresToDrop are missing in df\"\n",
    "    df2 = df.drop(featuresToDrop, axis=1)\n",
    "    \n",
    "    df2 = standardisation(df2, terms=list(df2.columns))\n",
    "    \n",
    "    pca = PCA()\n",
    "    pca_components = pca.fit_transform(df2)\n",
    "    \n",
    "    # find the explained variance ratio\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    print(\"Explained Variance Ratio: \", explained_variance)\n",
    "\n",
    "    # select number of components based on explained variance (e.g., 95% variance)\n",
    "    cumulative_variance = explained_variance.cumsum()\n",
    "    n_components = next(i for i, v in enumerate(cumulative_variance) if v >= 0.95) + 1\n",
    "    print(f\"Number of components to retain 95% variance: {n_components}\")\n",
    "\n",
    "    # visualize explained variance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(cumulative_variance, marker='o', linestyle='--')\n",
    "    plt.title('Cumulative Explained Variance by Number of Components')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    return n_components\n",
    "    \n",
    "number_of_components = pca(df)\n",
    "\n",
    "\n",
    "\n",
    "def chi_square(df):\n",
    "    # organise dataset into X (training examples) and y (targets)\n",
    "    featuresToDrop = ['Date', 'Home Team', 'Full Time Result', 'Away Team', 'Match Outcome']\n",
    "    assert set(featuresToDrop).issubset(df.columns), \"Some columns in featuresToDrop are missing in df\"\n",
    "    data = df.drop(featuresToDrop, axis=1)\n",
    "    df['Match Outcome'] = df['Full Time Result'].map({'H': 1, 'D': 0, 'A': -1})\n",
    "    target = df['Match Outcome']\n",
    "    \n",
    "    # chi2 scoring function requires non-negative input: normalisation\n",
    "    data = normalisation(data, terms=list(data.columns))\n",
    "    \n",
    "    # collect the feature names\n",
    "    feature_names = list(data.columns)\n",
    "    \n",
    "    # perform chi square selection    \n",
    "    chi_select = SelectKBest(chi2, k=min(number_of_components, len(data.columns)))\n",
    "    new_data = chi_select.fit_transform(data, target)\n",
    "    \n",
    "    # collect features\n",
    "    selected_features = []\n",
    "    for i, b in enumerate(chi_select.get_support()):\n",
    "        if b:\n",
    "            selected_features.append(feature_names[i])\n",
    "    \n",
    "    # get the chi-square scores for all features\n",
    "    chi_scores = chi_select.scores_\n",
    "\n",
    "    # create a dataFrame for easy visualization\n",
    "    chi2_df = pd.DataFrame({'Feature': feature_names, 'Chi-Square Score': chi_scores})\n",
    "    chi2_df = chi2_df.sort_values(by='Chi-Square Score', ascending=False)\n",
    "\n",
    "    # plot the scores\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    colours = ['skyblue' if i < number_of_components else 'gray' for i in range(len(chi2_df))]\n",
    "    plt.barh(chi2_df['Feature'], chi2_df['Chi-Square Score'], color=colours)\n",
    "    plt.xlabel('Chi-Square Score')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title('Feature Importance (Chi-Square)')\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis to show the highest scores at the top\n",
    "    plt.tight_layout()\n",
    "    plt.tick_params(axis=\"y\", pad=10, labelsize=5)\n",
    "    plt.show()\n",
    "    \n",
    "    return chi2_df\n",
    "    \n",
    "chi2_df = chi_square(updated_df)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_feature_weights(features, chi2_df):\n",
    "    # Aggregate Home and Away importance scores\n",
    "    aggregated_scores = {}\n",
    "    for feature_pair in features:\n",
    "        home_feature, away_feature = feature_pair\n",
    "        home_score = chi2_df.loc[chi2_df['Feature'] == home_feature, 'Chi-Square Score'].values[0]\n",
    "        away_score = chi2_df.loc[chi2_df['Feature'] == away_feature, 'Chi-Square Score'].values[0]\n",
    "        aggregated_scores[feature_pair] = home_score + away_score\n",
    "\n",
    "    # Sort features by aggregated importance\n",
    "    sorted_features = sorted(aggregated_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Adjust scores using the /3 rule\n",
    "    adjusted_scores = {}\n",
    "    adjusted_scores[sorted_features[0][0]] = sorted_features[0][1]  # Most important feature remains unchanged\n",
    "    \n",
    "    for i in range(1, len(sorted_features)):\n",
    "        current_feature, current_score = sorted_features[i]\n",
    "        previous_feature, previous_score = sorted_features[i - 1]\n",
    "        \n",
    "        if current_score < previous_score / 3:\n",
    "            current_score = previous_score / 3\n",
    "        adjusted_scores[current_feature] = current_score\n",
    "\n",
    "    # Assign adjusted scores equally to Home and Away features\n",
    "    final_weights = {}\n",
    "    for feature_pair, score in adjusted_scores.items():\n",
    "        home_feature, away_feature = feature_pair\n",
    "        final_weights[home_feature] = score / 2\n",
    "        final_weights[away_feature] = score / 2\n",
    "    \n",
    "    return final_weights\n",
    "\n",
    "def create_strength_features_separate(df, chi2_df):\n",
    "    # Define feature pairs for each group\n",
    "    attack_features = [\n",
    "        (\"Home Shots on Target\", \"Away Shots on Target\"),\n",
    "        (\"Home Progressive Passes\", \"Away Progressive Passes\"),\n",
    "        (\"Home Touches Att 3rd\", \"Away Touches Att 3rd\"),\n",
    "        (\"Home Take Ons Won\", \"Away Take Ons Won\"),\n",
    "        (\"Home Corners\", \"Away Corners\")\n",
    "    ]\n",
    "    midfield_features = [\n",
    "        (\"Home Touches Mid 3rd\", \"Away Touches Mid 3rd\"),\n",
    "        (\"Home Passes Completed\", \"Away Passes Completed\"),\n",
    "        (\"Home Passes PCT\", \"Away Passes PCT\"),\n",
    "        (\"Home Carries\", \"Away Carries\")\n",
    "    ]\n",
    "    defense_features = [\n",
    "        (\"Home Tackles\", \"Away Tackles\"),\n",
    "        (\"Home Tackles Won\", \"Away Tackles Won\"),\n",
    "        (\"Home Blocks\", \"Away Blocks\"),\n",
    "        (\"Home Interceptions\", \"Away Interceptions\")\n",
    "    ]\n",
    "    \n",
    "    # Adjust weights for each group\n",
    "    attack_weights = adjust_feature_weights(attack_features, chi2_df)\n",
    "    midfield_weights = adjust_feature_weights(midfield_features, chi2_df)\n",
    "    defense_weights = adjust_feature_weights(defense_features, chi2_df)\n",
    "    \n",
    "    # Calculate strength scores for Home and Away\n",
    "    def calculate_strength(feature_pairs, weights, team_type):\n",
    "        strength_score = 0\n",
    "        for home_feature, away_feature in feature_pairs:\n",
    "            feature = home_feature if team_type == \"Home\" else away_feature\n",
    "            strength_score += df[feature] * weights[feature]\n",
    "        return strength_score\n",
    "\n",
    "    # Compute Home and Away strengths for Attack, Midfield, and Defense\n",
    "    df['Home Attack Strength'] = calculate_strength(attack_features, attack_weights, \"Home\")\n",
    "    df['Away Attack Strength'] = calculate_strength(attack_features, attack_weights, \"Away\")\n",
    "    df['Home Midfield Strength'] = calculate_strength(midfield_features, midfield_weights, \"Home\")\n",
    "    df['Away Midfield Strength'] = calculate_strength(midfield_features, midfield_weights, \"Away\")\n",
    "    df['Home Defense Strength'] = calculate_strength(defense_features, defense_weights, \"Home\")\n",
    "    df['Away Defense Strength'] = calculate_strength(defense_features, defense_weights, \"Away\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "updated_df = create_strength_features_separate(updated_df, chi2_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove these 4 features\n",
    "updated_df['Home Goals Scored'] = updated_df['Full Time Home Goals']\n",
    "updated_df['Home Goals Conceded'] = updated_df['Full Time Away Goals']\n",
    "updated_df['Away Goals Scored'] = updated_df['Full Time Away Goals']\n",
    "updated_df['Away Goals Conceded'] = updated_df['Full Time Home Goals']\n",
    "updated_df[\"Home Performance\"] = updated_df[\"Match Outcome\"].apply(lambda x: 3 if x == 1 else 1 if x == 0 else 0)\n",
    "updated_df[\"Away Performance\"] = updated_df[\"Match Outcome\"].apply(lambda x: 3 if x == -1 else 1 if x == 0 else 0)\n",
    "\n",
    "updated_df['Home Avg Attacking Strength'] = updated_df.groupby('Home Team')['Home Attack Strength'].transform(lambda x: x.shift().rolling(window=5, min_periods=1).mean())\n",
    "updated_df['Home Avg Defense Strength'] = updated_df.groupby('Home Team')['Home Defense Strength'].transform(lambda x: x.shift().rolling(window=5, min_periods=1).mean())\n",
    "updated_df['Away Avg Attacking Strength'] = updated_df.groupby('Away Team')['Away Attack Strength'].transform(lambda x: x.shift().rolling(window=5, min_periods=1).mean())\n",
    "updated_df['Away Avg Defense Strength'] = updated_df.groupby('Away Team')['Away Defense Strength'].transform(lambda x: x.shift().rolling(window=5, min_periods=1).mean())\n",
    "updated_df['Home Avg Midfield Strength'] = updated_df.groupby('Home Team')['Home Midfield Strength'].transform(lambda x: x.shift().rolling(window=5, min_periods=1).mean())\n",
    "updated_df['Away Avg Midfield Strength'] = updated_df.groupby('Away Team')['Away Midfield Strength'].transform(lambda x: x.shift().rolling(window=5, min_periods=1).mean())\n",
    "updated_df['Home Recent Performance'] = (updated_df.groupby('Home Team')['Home Performance'].transform(lambda x: x.shift().rolling(window=5, min_periods=1).mean()))\n",
    "updated_df['Away Recent Performance'] = (updated_df.groupby('Away Team')['Away Performance'].transform(lambda x: x.shift().rolling(window=5, min_periods=1).mean()))\n",
    "\n",
    "# updated_df['Avg Home Goals Scored'] = (updated_df.groupby('Home Team')['Home Goals Scored'].transform(lambda x: x.shift().rolling(window=5, min_periods=1).mean()))\n",
    "# updated_df['Avg Home Goals Conceded'] = (updated_df.groupby('Home Team')['Home Goals Conceded'].transform(lambda x: x.shift().rolling(window=5, min_periods=1).mean()))\n",
    "# updated_df['Avg Away Goals Scored'] = (updated_df.groupby('Away Team')['Away Goals Scored'].transform(lambda x: x.shift().rolling(window=5, min_periods=1).mean()))\n",
    "# updated_df['Avg Away Goals Conceded'] = (updated_df.groupby('Away Team')['Away Goals Conceded'].transform(lambda x: x.shift().rolling(window=5, min_periods=1).mean()))\n",
    "# updated_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elo_ratings = {team: 1500 for team in pd.concat([updated_df['Home Team'], updated_df['Away Team']]).unique()}\n",
    "\n",
    "# Compute ELO ratings\n",
    "def compute_elo(row):\n",
    "    H_0, A_0 = elo_ratings[row['Home Team']], elo_ratings[row['Away Team']]\n",
    "    γ_H = 1 / (1 + 10 ** ((A_0 - H_0) / 400))\n",
    "    γ_A = 1 - γ_H\n",
    "    k = 30 * (1 + abs(row['Full Time Home Goals'] - row['Full Time Away Goals'])) ** 0.5\n",
    "    α_H = 1 if row['Full Time Home Goals'] > row['Full Time Away Goals'] else (0.5 if row['Full Time Home Goals'] == row['Full Time Away Goals'] else 0)\n",
    "    α_A = 1 - α_H\n",
    "    elo_ratings[row['Home Team']] = H_0 + k * (α_H - γ_H)\n",
    "    elo_ratings[row['Away Team']] = A_0 + k * (α_A - γ_A)\n",
    "    return pd.Series({'Home ELO': H_0, 'Away ELO': A_0})\n",
    "\n",
    "updated_df[['Home ELO', 'Away ELO']] = updated_df.apply(compute_elo, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "features = [\n",
    "    'Home ELO', 'Away ELO',\n",
    "    'Home Avg Attacking Strength', 'Away Avg Attacking Strength',\n",
    "    'Home Avg Defense Strength', 'Away Avg Defense Strength',\n",
    "    'Home Recent Performance', 'Away Recent Performance',\n",
    "    'Home Avg Midfield Strength', 'Away Avg Midfield Strength'\n",
    "]\n",
    "\n",
    "home_features = [f for f in features if 'Home' in f]\n",
    "away_features = [f for f in features if 'Away' in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "y_home_scored = updated_df['Full Time Home Goals']\n",
    "y_away_scored = updated_df['Full Time Away Goals']\n",
    "X_home_scored = updated_df[home_features]\n",
    "X_away_scored = updated_df[away_features]\n",
    "X_train_scored, X_test_scored, y_train_scored, y_test_scored = train_test_split(X_home_scored, y_home_scored, test_size=0.2, random_state=31)\n",
    "X_train_conceded, X_test_conceded, y_train_conceded, y_test_conceded = train_test_split(X_away_scored, y_away_scored, test_size=0.2, random_state=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_scored_model = RandomForestRegressor(random_state=31)\n",
    "away_scored_model = RandomForestRegressor(random_state=31)\n",
    "\n",
    "home_scored_model.fit(X_train_scored, y_train_scored)\n",
    "away_scored_model.fit(X_train_conceded, y_train_conceded)\n",
    "\n",
    "# Predictions\n",
    "home_scored_preds = home_scored_model.predict(X_test_scored)\n",
    "home_conceded_preds = away_scored_model.predict(X_test_conceded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Random Forest...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Home Avg Attacking Strength\n- Home Avg Defense Strength\n- Home Avg Midfield Strength\n- Home ELO\n- Home Recent Performance\nFeature names seen at fit time, yet now missing:\n- Away Avg Attacking Strength\n- Away Avg Defense Strength\n- Away Avg Midfield Strength\n- Away ELO\n- Away Recent Performance\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m away_scored_model\u001b[38;5;241m.\u001b[39mfit(X_train_conceded, y_train_conceded)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Predictions (use correct feature sets for each model)\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m home_scored_preds \u001b[38;5;241m=\u001b[39m \u001b[43mhome_scored_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_scored\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m away_conceded_preds \u001b[38;5;241m=\u001b[39m away_scored_model\u001b[38;5;241m.\u001b[39mpredict(X_test_conceded)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Metrics for goals\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\urasa\\Desktop\\UCL\\ML coursework\\venv\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:1063\u001b[0m, in \u001b[0;36mForestRegressor.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1061\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[1;32m-> 1063\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[0;32m   1066\u001b[0m n_jobs, _, _ \u001b[38;5;241m=\u001b[39m _partition_estimators(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n",
      "File \u001b[1;32mc:\\Users\\urasa\\Desktop\\UCL\\ML coursework\\venv\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:641\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    639\u001b[0m     force_all_finite \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 641\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc):\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\urasa\\Desktop\\UCL\\ML coursework\\venv\\lib\\site-packages\\sklearn\\base.py:608\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    539\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m    545\u001b[0m ):\n\u001b[0;32m    546\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \n\u001b[0;32m    548\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 608\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    610\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    611\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    612\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    613\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    614\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\urasa\\Desktop\\UCL\\ML coursework\\venv\\lib\\site-packages\\sklearn\\base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m    531\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    533\u001b[0m     )\n\u001b[1;32m--> 535\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Home Avg Attacking Strength\n- Home Avg Defense Strength\n- Home Avg Midfield Strength\n- Home ELO\n- Home Recent Performance\nFeature names seen at fit time, yet now missing:\n- Away Avg Attacking Strength\n- Away Avg Defense Strength\n- Away Avg Midfield Strength\n- Away ELO\n- Away Recent Performance\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import BayesianRidge, Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score, precision_score, f1_score, classification_report\n",
    "\n",
    "# Models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestRegressor(random_state=31),\n",
    "    \"K-Nearest Neighbors\": KNeighborsRegressor(n_neighbors=5),\n",
    "    \"Bayesian Regression\": BayesianRidge(),\n",
    "    \"Ridge Regression\": Ridge(alpha=1.0)\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    \n",
    "    # Train models for home and away\n",
    "    home_scored_model = model\n",
    "    away_scored_model = model\n",
    "    \n",
    "    # Fit models on the correct feature sets\n",
    "    home_scored_model.fit(X_train_scored, y_train_scored)\n",
    "    away_scored_model.fit(X_train_conceded, y_train_conceded)\n",
    "    \n",
    "    # Predictions (use correct feature sets for each model)\n",
    "    home_scored_preds = home_scored_model.predict(X_test_scored)\n",
    "    away_conceded_preds = away_scored_model.predict(X_test_conceded)\n",
    "    \n",
    "    # Metrics for goals\n",
    "    mae_scored = mean_absolute_error(y_test_scored, home_scored_preds)\n",
    "    mse_scored = mean_squared_error(y_test_scored, home_scored_preds)\n",
    "    mae_conceded = mean_absolute_error(y_test_conceded, away_conceded_preds)\n",
    "    mse_conceded = mean_squared_error(y_test_conceded, away_conceded_preds)\n",
    "    \n",
    "    # Outcome predictions\n",
    "    draw_threshold = 0.2\n",
    "    predicted_outcomes = []\n",
    "    for home_goals, away_goals in zip(home_scored_preds, away_conceded_preds):\n",
    "        goal_difference = abs(home_goals - away_goals)\n",
    "        if goal_difference < draw_threshold:\n",
    "            predicted_outcomes.append(0)  # Draw\n",
    "        elif home_goals > away_goals:\n",
    "            predicted_outcomes.append(1)  # Home Win\n",
    "        else:\n",
    "            predicted_outcomes.append(-1)  # Away Win\n",
    "\n",
    "    # Actual outcomes\n",
    "    actual_outcomes = updated_df.loc[X_test_scored.index, 'Match Outcome'].values  # Ensure indices match\n",
    "\n",
    "    # Classification metrics\n",
    "    accuracy = accuracy_score(actual_outcomes, predicted_outcomes)\n",
    "    precision = precision_score(actual_outcomes, predicted_outcomes, average='weighted')\n",
    "    f1 = f1_score(actual_outcomes, predicted_outcomes, average='weighted')\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision (Weighted): {precision:.2f}\")\n",
    "    print(f\"F1 Score (Weighted): {f1:.2f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(actual_outcomes, predicted_outcomes, target_names=['Away Win', 'Draw', 'Home Win']))\n",
    "\n",
    "    print(\"\\nHome Goals Scored Metrics:\")\n",
    "    print(f\"Home Scored: MAE: {mae_scored:.2f}, MSE: {mse_scored:.2f}\")\n",
    "\n",
    "    print(\"\\nHome Goals Conceded Metrics:\")\n",
    "    print(f\"Home Conceded MAE: {mae_conceded:.2f}, MSE: {mse_conceded:.2f}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.46\n",
      "Precision (Weighted): 0.47\n",
      "F1 Score (Weighted): 0.46\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Away Win       0.53      0.40      0.45       171\n",
      "        Draw       0.24      0.28      0.26       125\n",
      "    Home Win       0.54      0.60      0.57       248\n",
      "\n",
      "    accuracy                           0.46       544\n",
      "   macro avg       0.44      0.42      0.43       544\n",
      "weighted avg       0.47      0.46      0.46       544\n",
      "\n",
      "Home Goals Scored Metrics:\n",
      "Home Scored: MAE: 0.99, MSE: 1.65\n",
      "\n",
      "Home Goals Conceded Metrics:\n",
      "Home Conceded MAE: 0.99, MSE: 1.72\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, f1_score, classification_report\n",
    "# Evaluation\n",
    "\n",
    "mae_scored = mean_absolute_error(y_test_scored, home_scored_preds)\n",
    "mse_scored = mean_squared_error(y_test_scored, home_scored_preds)\n",
    "mae_conceded = mean_absolute_error(y_test_conceded, home_conceded_preds)\n",
    "mse_conceded = mean_squared_error(y_test_conceded, home_conceded_preds)\n",
    "\n",
    "draw_threshold = 0.2\n",
    "predicted_outcomes = []\n",
    "for home_goals, away_goals in zip(home_scored_preds, home_conceded_preds):\n",
    "    goal_difference = abs(home_goals - away_goals)\n",
    "    if goal_difference < draw_threshold:\n",
    "        predicted_outcomes.append(0)  # Draw\n",
    "    elif home_goals > away_goals:\n",
    "        predicted_outcomes.append(1)  # Home Win\n",
    "    else:\n",
    "        predicted_outcomes.append(-1)  # Away Win\n",
    "\n",
    "actual_outcomes = updated_df.loc[y_test_scored.index, 'Match Outcome'].values\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(actual_outcomes, predicted_outcomes)\n",
    "precision = precision_score(actual_outcomes, predicted_outcomes, average='weighted')\n",
    "f1 = f1_score(actual_outcomes, predicted_outcomes, average='weighted')\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision (Weighted): {precision:.2f}\")\n",
    "print(f\"F1 Score (Weighted): {f1:.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(actual_outcomes, predicted_outcomes, target_names=['Away Win', 'Draw', 'Home Win']))\n",
    "\n",
    "print(\"Home Goals Scored Metrics:\")\n",
    "print(f\"Home Scored: MAE: {mae_scored:.2f}, MSE: {mse_scored:.2f}\")\n",
    "\n",
    "print(\"\\nHome Goals Conceded Metrics:\")\n",
    "print(f\"Home Conceded MAE: {mae_conceded:.2f}, MSE: {mse_conceded:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
